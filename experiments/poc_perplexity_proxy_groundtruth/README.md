# Perplexity Proxy Groundtruth Experiment

This experiment aims to evaluate the use of perplexity as a proxy for retrieval-augmented generation (RAG) model performance. The core idea is to assess whether lower perplexity scores on a language model, given a set of retrieved documents, correlate with higher-quality generated answers.

## Folder Structure

The project is organized into the following directories:

- **`experiments_.../`**: Each experiment has its own directory, identified by a unique seed.
    - **`datamodels/`**: Contains the data models and collections for the experiment.
        - **`collections/`**: Contains the generated collections of documents for each experiment.
        - **`models/`**: Contains the trained models.
    - **`generations/`**: Contains the generated answers for each run type.
    - **`retrieval/`**: Contains the retrieval indexes used by the RAG models.
- **`results/`**: Stores the evaluation results of the experiments in Feather format.
- **`normalize_perplexity.py`**: Script for normalizing perplexity scores.
- **`proxy_perplexity.ipynb`**: Notebook for perplexity analysis.
- **`run_pipeline.py`**: Main script for executing the different stages of the experiment.
- **`save_perplexity_collections.py`**: Script for calculating and saving perplexity scores.
- **`run.sh`**: Shell script to run the pipeline.

## Scripts

- **`normalize_perplexity.py`**: This script is responsible for normalizing the perplexity scores generated by `save_perplexity_collections.py`. It takes the raw perplexity values and applies a min-max normalization technique. This is done to scale the perplexity scores to a common range, making them more comparable across different test instances. The normalization is performed group-wise for each `test_idx`, ensuring that the scaling is relative to the perplexity scores of a specific question.

- **`save_perplexity_collections.py`**: This script is the core of the perplexity calculation process. It iterates through the test dataset and, for each question, calculates the perplexity of a large language model (Llama-3.2-3B-Instruct) when conditioned on different sets of retrieved documents. The script loads the necessary data, including the document collections, the wiki text, the questions, and the retrieval indexes. It then uses the `calculate_batch_perplexity` function to compute the perplexity scores and saves them in a Feather file.

- **`run_pipeline.py`**: This script acts as the main entry point for the entire experiment. It orchestrates the different stages of the pipeline, from setting up the environment to training the models and generating the final results. The script is highly configurable through the `ParametersConfig` data class, which allows for easy modification of the experiment's parameters. The pipeline is divided into three main parts:
    - **`datamodels_setup`**: This stage prepares the environment for the experiment. It copies the necessary files, such as the collections and retrieval indexes, to the experiment's directory.
    - **`datamodels_training`**: In this stage, the script trains the data models using the pre-processed collections.
    - **`datamodels_generations`**: This final stage uses the trained models to generate answers and saves them for evaluation.

## How to run it

There are two ways to run the experiments: using the `run.sh` script or by running the commands individually.

### Using the `run.sh` script

The `run.sh` script is the easiest way to run the entire pipeline for all the seeds. It will execute all the necessary steps, from calculating the perplexity to generating the final results.

To run the script, simply execute the following command in your terminal:

```bash
bash run.sh
```

### Running the commands individually

You can also run the commands individually if you want to have more control over the process. The following commands are executed by the `run.sh` script for each seed.

1.  **Calculate perplexity without instruction:**

    ```bash
    python save_perplexity_collections.py --seed <your_seed> --start_idx 0 --end_idx 50 --optional_instruction "" --saving_prefix non_normalized_perplexity_baseline
    ```

2.  **Calculate perplexity with instruction:**

    ```bash
    python save_perplexity_collections.py --seed <your_seed> --start_idx 0 --end_idx 50 --saving_prefix non_normalized_perplexity_collections
    ```

3.  **Normalize perplexity:**

    ```bash
    python normalize_perplexity.py --seed <your_seed> --target_prefix non_normalized_perplexity_collections --saving_prefix normalized_perplexity_baseline
    ```

4.  **Train the data models with instruction:**

    ```bash
    python run_pipeline.py --seed <your_seed> --run_type datamodels_training  --model_run_id perplexity_with_instruction --train_collection_id perplexity_with_instruction
    ```

5.  **Generate answers with the data models with instruction:**

    ```bash
    python run_pipeline.py --seed <your_seed> --run_type datamodels_generations  --model_run_id perplexity_with_instruction --train_collection_id perplexity_with_instruction
    ```

6.  **Train the data models without instruction:**

    ```bash
    python run_pipeline.py --seed <your_seed> --run_type datamodels_training --model_run_id perplexity_baseline --train_collection_id perplexity_baseline
    ```

7.  **Generate answers with the data models without instruction:**

    ```bash
    python run_pipeline.py --seed <your_seed> --run_type datamodels_generations --model_run_id perplexity_baseline --train_collection_id perplexity_baseline
    ```

## Results

The results of the experiments are stored in the `results/` directory. The `preview_evaluation.ipynb` notebook contains a preliminary analysis of the results. The key findings are as follows:

### ROUGE-L Scores

The following table summarizes the mean ROUGE-L scores for the different experiment runs:

| Seed | Run Type | Mean ROUGE-L |
|---|---|---|
| 5191 | baseline | 0.117714 |
| 5191 | datamodels | 0.58269 |
| 5191 | perplexity_with_instruction | 0.01641 |
| 5191 | rag | 0.173952 |
| 5734 | baseline | 0.149668 |
| 5734 | datamodels | 0.69123 |
| 5734 | perplexity_with_instruction | 0.01493 |
| 5734 | rag | 0.19233 |
| 7270 | baseline | 0.286462 |
| 7270 | datamodels | 0.606403 |
| 7270 | perplexity_baseline | 0.006154 |
| 7270 | perplexity_with_instruction | 0.010909 |
| 7270 | rag | 0.255143 |
| 860 | baseline | 0.165779 |
| 860 | datamodels | 0.735651 |
| 860 | perplexity_baseline | 0.003077 |
| 860 | perplexity_with_instruction | 0.010909 |
| 860 | rag | 0.202825 |

As shown in the table, the `datamodels` run consistently achieves the highest ROUGE-L scores across all seeds, indicating that the data models significantly improve the quality of the generated answers. The `perplexity` runs, on the other hand, perform poorly, which is expected since it is not a generation model but a metric. The `rag` and `baseline` runs show similar performance, with the `rag` model having a slight edge.