{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "866ac57e",
   "metadata": {},
   "source": [
    "# Testing Implementation\n",
    "\n",
    "The goal of this notebook is to mock one response generation and single grading to undestand the results and how to parser everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9c946cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caio.rhoden/miniconda3/envs/nq_dl/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dmcr.models import GenericInstructModelHF\n",
    "from pathlib import Path\n",
    "from utils.set_random_seed import set_random_seed\n",
    "import os\n",
    "import re\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "set_random_seed(42)\n",
    "root = \"../..\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5519f355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s]\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "llm = GenericInstructModelHF(path=f\"{root}/models/Llama-3.2-3B-Instruct\")\n",
    "configs = {\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_new_tokens\": 10,\n",
    "    \"top_p\": 0.9\n",
    "}\n",
    "\n",
    "question = \"when did veterans day start being called veterans day?\"\n",
    "instruction = \"Answer the question in 5 tokens or less.\"\n",
    "prompt = f\"{question}\\n\"\n",
    "response = llm.run(prompt=prompt, instruction=instruction, config_params=configs)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb1397dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1944, but it wasn't officially called Veterans\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94126252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\n[System] \\nPlease act as an impartial judge and evaluate the quality of the response provided by an AI assistant to a question displayed below. Your evaluation should consider factors such as relevance and accuracy. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, please rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".  \\n[Question] \\nwhen did veterans day start being called veterans day?  \\n[The Start of Assistant’s Answer] \\n1944, but it wasn\\'t officially called Veterans\\n[The End of Assistant’s Answer]\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_prompt = f\"\"\"\"\n",
    "[System] \n",
    "Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to a question displayed below. Your evaluation should consider factors such as relevance and accuracy. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, please rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".  \n",
    "[Question] \n",
    "{question}  \n",
    "[The Start of Assistant’s Answer] \n",
    "{response}\n",
    "[The End of Assistant’s Answer]\n",
    "\"\"\"\n",
    "evaluation_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b47d6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "configs = {\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.9,\n",
    "    \"num_return_sequences\": 5,\n",
    "}\n",
    "grades = llm.run(prompt=evaluation_prompt, instruction=\"\", config_params=configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8c6d4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"In evaluating the quality of the response provided by the AI assistant, I consider factors such as relevance, accuracy, completeness, and clarity. The response should directly address the question asked and provide a clear and concise answer. In this case, the AI assistant's response attempts to answer the question about when Veterans Day started being called Veterans Day, but it falls short in several areas.\\n\\nThe response starts by providing a year (1944), which is accurate in the sense that World War II ended in 1945, but it was in 1954 that the U.S. government officially declared November 11 as Veterans Day. However, the response does not provide the correct information about when it was officially called Veterans Day, which is a significant omission. Furthermore, the response is incomplete, as it does not mention the significance of the Armistice Day holiday that was celebrated on November 11 prior to 1954.\\n\\nRating: [[4]]\"},\n",
       " {'generated_text': 'I\\'ll evaluate the response based on its relevance and accuracy to the question asked. The question inquires about when Veterans Day started being called by that name, implying an interest in the evolution of the holiday\\'s nomenclature. The response provided by the AI assistant mentions 1944, which is correct in the sense that the holiday originated in 1944 as Armistice Day, but it does not address the specific question about when it started being called \"Veterans Day.\"\\n\\nThe response lacks clarity and directly answers a different aspect of the holiday\\'s history, which might confuse the user. To provide a more accurate answer to the user\\'s question, the assistant could have mentioned that the holiday was officially renamed Veterans Day in 1954, following a congressional act that amended the Act of 1944.\\n\\nRating: [[4]]'},\n",
       " {'generated_text': \"I'll evaluate the response based on its relevance and accuracy.\\n\\nThe AI assistant's response partially addresses the question, providing a year when Veterans Day started being observed, but it is incomplete and inaccurate in its wording. The year 1944 is correct, as it marks the beginning of the modern American Veterans Day celebration, which was established as Armistice Day to commemorate the end of World War I. However, the response fails to mention that it was officially renamed Veterans Day in 1954.\\n\\nRating: [[6]]\"},\n",
       " {'generated_text': \"Evaluation Explanation:\\n\\nIn this evaluation, I will assess the quality of the AI assistant's response to the question about when Veterans Day started being called Veterans Day. The factors considered include relevance and accuracy. Relevance refers to how well the response addresses the question posed, while accuracy pertains to the correctness of the information provided.\\n\\nUpon reviewing the response, I observed that it partially addresses the question but lacks essential details. The assistant correctly states that 1944 is the year when the holiday began, but it does not provide further context or clarify why it was initially called something other than Veterans Day.\\n\\nRating: [[6]]\"},\n",
       " {'generated_text': \"Evaluation Explanation:\\n\\nIn this evaluation, I will assess the quality of the AI assistant's response to the question about when Veterans Day started being called Veterans Day. I will consider factors such as relevance, accuracy, completeness, and clarity of the response.\\n\\nThe AI assistant's response provides a specific year, 1944, which is a relevant date in the history of Veterans Day. However, the response is incomplete, as it only mentions 1944 without explaining why it wasn't officially called Veterans Day at that time. Additionally, the response lacks clarity and context, which may lead to confusion among users.\\n\\nRating: [[6]]\"}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd712136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rating_mean(data):\n",
    "    \"\"\"\n",
    "    Calculate the mean rating from generated texts containing 'Rating: [[number]]' patterns.\n",
    "    \n",
    "    Args:\n",
    "        data: List of dictionaries with 'generated_text' keys\n",
    "        \n",
    "    Returns:\n",
    "        Mean rating (float), or None if no ratings found\n",
    "    \"\"\"\n",
    "    ratings = []\n",
    "    \n",
    "    for item in data:\n",
    "        text = item['generated_text']\n",
    "        # Search for rating pattern\n",
    "        match = re.search(r'Rating: \\[\\[(\\d+)\\]\\]', text)\n",
    "        if match:\n",
    "            ratings.append(int(match.group(1)))\n",
    "    \n",
    "    if not ratings:\n",
    "        return None\n",
    "    \n",
    "    return sum(ratings) / len(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f66fe7f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_rating_mean(grades)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nq_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
