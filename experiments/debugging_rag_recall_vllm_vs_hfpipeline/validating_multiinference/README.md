
# Validating Multi-inference Datamodels

The goal of this limited experiment is to have a validation benchmark for the perfromance of the datamodels re-ranker when applying the multi-inference. The expectations with this approach is to achieve a more robust metric for the linear models to be trained on.

## Experiment Setup

* There are five different subsets copied from "validation_datamodels" subfolder on the same level as this one. They are the same to be directly comperable
* The evaluation will be on Rouge-L metric and XX. They will be evaluated in Rouge-l groundtruth and Judge unsupervised apporaches.
* It will be tested for each experient multiple sentence inference with 3 and 5 sentences.


## How to run it?

## Results



