# Perplexity Proxy Groundtruth Experiment

This experiment aims to evaluate the use of perplexity as a proxy for retrieval-augmented generation (RAG) model performance. The core idea is to assess whether lower perplexity scores on a language model, given a set of retrieved documents, correlate with higher-quality generated answers.

## Folder Structure

The project is organized into the following directories:

- **`collections/`**: Contains the generated collections of documents for each experiment, identified by a unique seed. These collections are stored in Feather format.
- **`collections_dataset/`**: Holds the dataset versions of the collections in HDF5 format, which are used for training and testing the models.
- **`dmrc_pipelines/`**: Contains the main pipeline scripts for running the experiments.
  - **`run_pipeline.py`**: The main script for executing the different stages of the experiment (setup, training, generation).
- **`results/`**: Stores the evaluation results of the experiments in Feather format.
- **`retrieval/`**: Contains the retrieval indexes used by the RAG models.

## Scripts

- **`normalize_perplexity.py`**: This script is responsible for normalizing the perplexity scores generated by `save_perplexity_collections.py`. It takes the raw perplexity values and applies a min-max normalization technique. This is done to scale the perplexity scores to a common range, making them more comparable across different test instances. The normalization is performed group-wise for each `test_idx`, ensuring that the scaling is relative to the perplexity scores of a specific question.

- **`save_perplexity_collections.py`**: This script is the core of the perplexity calculation process. It iterates through the test dataset and, for each question, calculates the perplexity of a large language model (Llama-3.2-3B-Instruct) when conditioned on different sets of retrieved documents. The script loads the necessary data, including the document collections, the wiki text, the questions, and the retrieval indexes. It then uses the `calculate_batch_perplexity` function to compute the perplexity scores and saves them in a Feather file.

- **`dmrc_pipelines/run_pipeline.py`**: This script acts as the main entry point for the entire experiment. It orchestrates the different stages of the pipeline, from setting up the environment to training the models and generating the final results. The script is highly configurable through the `ParametersConfig` data class, which allows for easy modification of the experiment's parameters. The pipeline is divided into three main parts:
    - **`datamodels_setup`**: This stage prepares the environment for the experiment. It copies the necessary files, such as the collections and retrieval indexes, to the experiment's directory.
    - **`datamodels_training`**: In this stage, the script trains the data models using the pre-processed collections.
    - **`datamodels_generations`**: This final stage uses the trained models to generate answers and saves them for evaluation.

## Usage

To run the experiment, you can use the `dmrc_pipelines/run_pipeline.py` script with different arguments. The `run_type` argument specifies which part of the pipeline to execute.

### Setup

To set up the experiment, run the following command:

```bash
python dmrc_pipelines/run_pipeline.py --run_type="datamodels_setup" --seed=<your_seed>
```

### Training

To train the data models, use the following command:

```bash
python dmrc_pipelines/run_pipeline.py --run_type="datamodels_training" --seed=<your_seed>
```

### Generation

To generate answers using the trained models, run:

```bash
python dmrc_pipelines/run_pipeline.py --run_type="datamodels_generations" --seed=<your_seed>
```

## Results

The results of the experiments are stored in the `results/` directory. The `preview_evaluation.ipynb` notebook contains a preliminary analysis of the results. The key findings are as follows:

### ROUGE-L Scores

The following table summarizes the mean ROUGE-L scores for the different experiment runs:

| Seed | Run Type   | Mean ROUGE-L |
|------|------------|--------------|
| 7270 | baseline   | 0.286462     |
| 7270 | datamodels | 0.606403     |
| 7270 | perplexity | 0.006154     |
| 7270 | rag        | 0.255143     |
| 860  | baseline   | 0.165779     |
| 860  | datamodels | 0.735651     |
| 860  | perplexity | 0.003077     |
| 860  | rag        | 0.202825     |

As shown in the table, the `datamodels` run consistently achieves the highest ROUGE-L scores for both seeds, indicating that the data models significantly improve the quality of the generated answers. The `perplexity` run, on the other hand, performs poorly, which is expected since it is not a generation model but a metric.

### Retrieval Intersection

The notebook also analyzes the intersection of retrieved documents between the different retrieval strategies. The results show that there is a considerable overlap between the documents retrieved by the RAG, perplexity, and ROUGE-based methods. This suggests that the different methods are selecting similar documents, which is a positive sign for the validity of the perplexity-based approach.
