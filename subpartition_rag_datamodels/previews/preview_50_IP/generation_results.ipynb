{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33995338",
   "metadata": {},
   "source": [
    "# Results Analysis\n",
    "\n",
    "The goal of this notebook is to compare the text generations when using the traditional RAG and when usign the re-ranking with datamodels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04d0f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dmcr.evaluators.Rouge_L_evaluator import Rouge_L_evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a198788",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09c36eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATIONS_PATH = \"generations\"\n",
    "rag_generations = json.load(open(f\"{GENERATIONS_PATH}/generations.json\"))\n",
    "datamodels_generations = json.load(open(f\"{GENERATIONS_PATH}/datamodels_generations.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ccb888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = pl.read_ipc(\"questions.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff50646d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['According to the Death Penalty Information Center, there are'],\n",
       "      dtype='<U60')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([datamodels_generations[str(0)][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7969383d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['seven'], dtype='<U5')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0805686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 'There are 7 episodes in Big Little Lies Season',\n",
       " 'Big Little Lies Season 2 has 7 episodes',\n",
       " 'There are 7 episodes in Big Little Lies Season',\n",
       " 'Big Little Lies Season 2 has 7 episodes']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodels_generations[str(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd152cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair: 2,718 - I couldn't find any information related to the total\n",
      "Pair: 2,718 - I couldn't find any information related to the question\n",
      "Pair: 2,718 - According to the FBI's data, there are approximately\n",
      "Pair: 2,718 - I couldn't find any information related to the total\n",
      "Pair: 2,718 - I don't have any information about death row inmates\n",
      "Pair: seven - There are 7 episodes in Big Little Lies Season\n",
      "Pair: seven - There are 7 episodes in Season 2 of\n",
      "Pair: seven - Big Little Lies Season 2 has 7 episodes\n",
      "Pair: seven - There are 7 episodes in Big Little Lies Season\n",
      "Pair: seven - Big Little Lies Season 2 has 7 episodes\n",
      "Pair: Foreigner - Sam Bailey\n",
      "Pair: Foreigner - Sam Bailey\n",
      "Pair: Foreigner - Sam Bailey\n",
      "Pair: Foreigner - Sam Bailey\n",
      "Pair: Foreigner - Sam Bailey\n",
      "Pair: Saltfjellet - The Norwegian island of Kapan.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m metric_rag = evaluator.evaluate(np.array([res]), np.array([\u001b[38;5;28mstr\u001b[39m(rag_generations[\u001b[38;5;28mstr\u001b[39m(i)][j])]))\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPair: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrag_generations[\u001b[38;5;28mstr\u001b[39m(i)][j]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m metric_datamodels = \u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mres\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdatamodels_generations\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m max_rag = \u001b[38;5;28mmax\u001b[39m(max_rag, metric_rag[\u001b[32m0\u001b[39m])\n\u001b[32m     25\u001b[39m max_datamodels = \u001b[38;5;28mmax\u001b[39m(max_datamodels, metric_datamodels[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nq/lib/python3.11/site-packages/dmcr/evaluators/Rouge_L_evaluator.py:36\u001b[39m, in \u001b[36mRouge_L_evaluator.evaluate\u001b[39m\u001b[34m(self, y, y_pred)\u001b[39m\n\u001b[32m     34\u001b[39m max_result = \u001b[32m0\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ref_i \u001b[38;5;129;01min\u001b[39;00m ref:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrouge_l\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mref_i\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     max_result = \u001b[38;5;28mmax\u001b[39m(result[\u001b[33m\"\u001b[39m\u001b[33mrougeL\u001b[39m\u001b[33m\"\u001b[39m], max_result)\n\u001b[32m     39\u001b[39m results.append(max_result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nq/lib/python3.11/site-packages/evaluate/module.py:467\u001b[39m, in \u001b[36mEvaluationModule.compute\u001b[39m\u001b[34m(self, predictions, references, **kwargs)\u001b[39m\n\u001b[32m    465\u001b[39m inputs = {input_name: \u001b[38;5;28mself\u001b[39m.data[input_name] \u001b[38;5;28;01mfor\u001b[39;00m input_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._feature_names()}\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m temp_seed(\u001b[38;5;28mself\u001b[39m.seed):\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcompute_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.buf_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    470\u001b[39m     \u001b[38;5;28mself\u001b[39m.buf_writer = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--rouge/b01e0accf3bd6dd24839b769a5fda24e14995071570870922c71970b3a6ed886/rouge.py:149\u001b[39m, in \u001b[36mRouge._compute\u001b[39m\u001b[34m(self, predictions, references, rouge_types, use_aggregator, use_stemmer, tokenizer)\u001b[39m\n\u001b[32m    146\u001b[39m         scores.append(score)\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_aggregator:\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     result = \u001b[43maggregator\u001b[49m\u001b[43m.\u001b[49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[32m    151\u001b[39m         result[key] = result[key].mid.fmeasure\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nq/lib/python3.11/site-packages/rouge_score/scoring.py:124\u001b[39m, in \u001b[36mBootstrapAggregator.aggregate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    122\u001b[39m score_matrix = np.vstack(\u001b[38;5;28mtuple\u001b[39m(scores))\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Percentiles are returned as (interval, measure).\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m percentiles = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bootstrap_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# Extract the three intervals (low, mid, high).\u001b[39;00m\n\u001b[32m    126\u001b[39m intervals = \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m    127\u001b[39m     (scores[\u001b[32m0\u001b[39m].\u001b[34m__class__\u001b[39m(*percentiles[j, :]) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nq/lib/python3.11/site-packages/rouge_score/scoring.py:150\u001b[39m, in \u001b[36mBootstrapAggregator._bootstrap_resample\u001b[39m\u001b[34m(self, matrix)\u001b[39m\n\u001b[32m    147\u001b[39m sample_mean = np.zeros((\u001b[38;5;28mself\u001b[39m._n_samples, matrix.shape[\u001b[32m1\u001b[39m]))\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m._n_samples):\n\u001b[32m    149\u001b[39m   sample_idx = np.random.choice(\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m       np.arange(matrix.shape[\u001b[32m0\u001b[39m]), size=matrix.shape[\u001b[32m0\u001b[39m])\n\u001b[32m    151\u001b[39m   sample = matrix[sample_idx, :]\n\u001b[32m    152\u001b[39m   sample_mean[i, :] = np.mean(sample, axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"idx\": [],\n",
    "    \"mean_metric_rag\": [],\n",
    "    \"mean_metric_datamodels\": [],\n",
    "}\n",
    "evaluator = Rouge_L_evaluator()\n",
    "\n",
    "\n",
    "for i in range(len(rag_generations)):\n",
    "    rag_i_results = []\n",
    "    datamodels_i_results = []\n",
    "    results[\"idx\"].append(i)\n",
    "    for j in range(len(rag_generations[str(i)])):\n",
    "\n",
    "        max_rag = 0\n",
    "        max_datamodels = 0\n",
    "        \n",
    "        for res in  questions[i][\"answers\"].to_numpy().flatten()[0]:\n",
    "\n",
    "            metric_rag = evaluator.evaluate(np.array([res]), np.array([str(rag_generations[str(i)][j])]))\n",
    "            metric_datamodels = evaluator.evaluate(np.array([res]), np.array([str(datamodels_generations[str(i)][j])]))\n",
    "            \n",
    "            max_rag = max(max_rag, metric_rag[0])\n",
    "            max_datamodels = max(max_datamodels, metric_datamodels[0])\n",
    "\n",
    "        rag_i_results.append(max_rag)\n",
    "        datamodels_i_results.append(max_datamodels)\n",
    "\n",
    "    results[\"mean_metric_rag\"].append(np.mean(rag_i_results))\n",
    "    results[\"mean_metric_datamodels\"].append(np.mean(datamodels_i_results))\n",
    "    df_results = pl.DataFrame(results)\n",
    "    df_results.write_ipc(\"results.feather\")\n",
    "\n",
    "df_results = pl.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c5b1a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the Death Penalty Information Center, there are'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_generations[str(i)][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cf4a4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2,718'], dtype='<U5')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b298b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
